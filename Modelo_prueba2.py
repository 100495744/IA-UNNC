import os
import re
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import json
import random
from collections import Counter

# ===============================
# CONFIG
# ===============================
DATA_DIR = "Content"
CHECKPOINT_DIR = "checkpoints"
OUTPUT_DIR = "generated_stories"
os.makedirs(CHECKPOINT_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Device: {DEVICE}")

# ===============================
# 1. CARGA Y PROCESAMIENTO DE DATOS
# ===============================
print("\n=== PROCESANDO DATOS ===")

def load_and_process_texts():
    """Carga todos los textos y extrae patrones de inicio de oraciones"""
    all_text = ""
    sentence_starts = []  # Inicios de oraciones para generar desde cero
    
    files = [f for f in os.listdir(DATA_DIR) if f.endswith('.txt')]
    if not files:
        print(f"Error: No hay archivos .txt en {DATA_DIR}")
        return "", [], []
    
    for file in files[:3]:  # Usar primeros 3 archivos
        path = os.path.join(DATA_DIR, file)
        try:
            with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                text = f.read()
                all_text += text + " "
                
                # Extraer inicios de oraciones
                sentences = re.split(r'[.!?]+', text)
                for sent in sentences:
                    sent = sent.strip()
                    if len(sent) > 20:  # Oraciones significativas
                        # Tomar primeras 3-5 palabras
                        words = sent.split()[:5]
                        if len(words) >= 3:
                            sentence_starts.append(' '.join(words).lower())
                            
        except Exception as e:
            print(f"Error con {file}: {e}")
    
    return all_text, sentence_starts[:1000], files[:3]  # Limitar a 1000 inicios

# Cargar datos
raw_text, sentence_starts, used_files = load_and_process_texts()
print(f"Texto cargado: {len(raw_text):,} caracteres")
print(f"Patrones de inicio: {len(sentence_starts)}")
print(f"Archivos usados: {', '.join(used_files)}")

# ===============================
# 2. CONSTRUIR VOCABULARIO
# ===============================
def build_vocab(text, min_freq=3):
    """Construye vocabulario desde el texto"""
    # Tokenizar
    text = text.lower()
    tokens = re.findall(r'\b[a-z]+\b|[.,!?;:]', text)
    
    # Contar frecuencias
    word_freq = Counter(tokens)
    
    # Crear vocabulario
    vocab = ['<PAD>', '<UNK>', '<SOS>', '<EOS>'] + \
            [word for word, freq in word_freq.items() if freq >= min_freq]
    
    word2idx = {word: idx for idx, word in enumerate(vocab)}
    idx2word = {idx: word for word, idx in word2idx.items()}
    
    # Estad√≠sticas
    print(f"\nVocabulario: {len(vocab)} palabras")
    print(f"Tokens totales: {len(tokens):,}")
    print(f"Palabras √∫nicas: {len(set(tokens)):,}")
    
    # Mostrar palabras m√°s comunes
    print("\n20 palabras m√°s comunes:")
    for word, freq in word_freq.most_common(20):
        print(f"  {word}: {freq}")
    
    return vocab, word2idx, idx2word

# Construir o cargar vocabulario
vocab_file = os.path.join(DATA_DIR, "story_vocab.json")
if os.path.exists(vocab_file):
    with open(vocab_file, 'r') as f:
        word2idx = json.load(f)
    idx2word = {int(idx): word for word, idx in word2idx.items()}
    print(f"\nVocabulario cargado: {len(word2idx)} palabras")
else:
    print("\nConstruyendo vocabulario...")
    vocab, word2idx, idx2word = build_vocab(raw_text, min_freq=3)
    
    # Guardar vocabulario
    with open(vocab_file, 'w') as f:
        json.dump(word2idx, f)
    print(f"Vocabulario guardado en {vocab_file}")

vocab_size = len(word2idx)

# ===============================
# 3. CODIFICAR TEXTO
# ===============================
def text_to_indices(text, word2idx):
    """Convierte texto a √≠ndices num√©ricos"""
    tokens = re.findall(r'\b[a-z]+\b|[.,!?;:]', text.lower())
    return [word2idx.get(token, word2idx['<UNK>']) for token in tokens]

encoded_text = text_to_indices(raw_text, word2idx)
print(f"\nTexto codificado: {len(encoded_text):,} tokens")

# ===============================
# 4. DATASET
# ===============================
class AutoStoryDataset(Dataset):
    """Dataset para entrenamiento autom√°tico de historias"""
    def __init__(self, encoded_text, seq_length=50):
        self.data = encoded_text
        self.seq_length = seq_length
        self.num_sequences = len(encoded_text) - seq_length
        
    def __len__(self):
        return self.num_sequences
    
    def __getitem__(self, idx):
        # Secuencia de entrada
        x = self.data[idx:idx + self.seq_length]
        
        # Target (siguiente token)
        y = self.data[idx + self.seq_length]
        
        return torch.tensor(x), torch.tensor(y)

# ===============================
# 5. MODELO MEJORADO PARA GENERACI√ìN AUT√ìNOMA
# ===============================
class AutonomousStoryGenerator(nn.Module):
    """Modelo que puede generar historias sin prompts"""
    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512, num_layers=2):
        super().__init__()
        
        # Embeddings
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        
        # LSTM principal
        self.lstm = nn.LSTM(
            embed_dim,
            hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.3 if num_layers > 1 else 0,
            bidirectional=False
        )
        
        # Capas de atenci√≥n
        self.attention = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.Tanh(),
            nn.Linear(hidden_dim // 2, 1)
        )
        
        # Capas finales
        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, vocab_size)
        
        # Dropout
        self.dropout = nn.Dropout(0.4)
        
        # Normalizaci√≥n
        self.layer_norm = nn.LayerNorm(hidden_dim)
        
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
    def forward(self, x, hidden=None):
        batch_size = x.size(0)
        
        # Embedding
        embedded = self.dropout(self.embedding(x))  # [batch, seq_len, embed_dim]
        
        # LSTM
        lstm_out, hidden = self.lstm(embedded, hidden)  # [batch, seq_len, hidden_dim]
        lstm_out = self.layer_norm(lstm_out)
        
        # Atenci√≥n
        attention_weights = torch.softmax(
            self.attention(lstm_out).squeeze(-1), 
            dim=1
        )  # [batch, seq_len]
        
        # Contexto
        context = torch.bmm(
            attention_weights.unsqueeze(1),  # [batch, 1, seq_len]
            lstm_out  # [batch, seq_len, hidden_dim]
        ).squeeze(1)  # [batch, hidden_dim]
        
        # √öltima salida LSTM
        last_out = lstm_out[:, -1, :]  # [batch, hidden_dim]
        
        # Combinar
        combined = torch.cat([last_out, context], dim=1)  # [batch, hidden_dim * 2]
        
        # Capas finales
        output = self.fc1(combined)
        output = self.dropout(torch.relu(output))
        output = self.fc2(output)
        
        return output, hidden
    
    def init_hidden(self, batch_size):
        """Inicializa estados ocultos"""
        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(DEVICE),
                torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(DEVICE))

# ===============================
# 6. GENERACI√ìN AUT√ìNOMA (SIN PROMPTS)
# ===============================
def generate_autonomous_story(model, word2idx, idx2word, length=500, 
                             temperature=0.7, diversity=0.8, max_paragraphs=4):
    """
    Genera una historia completa SIN prompts, desde cero
    """
   # model.eval()
    
    seed_text = generate_random_story_start(sentence_starts, word2idx)
    generated_indices = text_to_indices(seed_text, word2idx)
    current_token = torch.tensor([[generated_indices[-1]]]).to(DEVICE)
    generated = generated_indices
    
    hidden = None
    
    with torch.no_grad():
        for step in range(length):
            # Forward pass
            if hidden is None:
                logits, hidden = model(current_token)
            else:
                logits, hidden = model(current_token, hidden)
            
            # Aplicar temperatura y diversidad
            logits = logits / temperature
            
            # Penalizaci√≥n de repetici√≥n
            if len(generated) > 10:
                recent = generated[-10:]
                for token in set(recent):
                    count = recent.count(token)
                    if count > 2:
                        logits[0, token] -= diversity * count
            
            # Softmax y sampling
            probs = torch.softmax(logits, dim=-1).squeeze()
            
            # Evitar tokens especiales durante generaci√≥n
            for special_token in ['<PAD>', '<UNK>', '<SOS>']:
                if special_token in word2idx:
                    probs[word2idx[special_token]] = 0
            
            # Muestreo
            next_token = torch.multinomial(probs, 1).item()
            generated.append(next_token)
            
            # Obtener palabra
            word = idx2word.get(next_token, '')
            
            # Manejar fin de oraci√≥n
            if word in '.!?':
                current_paragraph.append(word)
                
                # Decidir si terminar p√°rrafo
                sentences_in_paragraph += 1
                if sentences_in_paragraph >= random.randint(3, 6):
                    # Fin de p√°rrafo
                    paragraph_text = ' '.join(current_paragraph)
                    story_parts.append(paragraph_text)
                    current_paragraph = []
                    sentences_in_paragraph = 0
                    paragraphs_generated += 1
                    
                    if paragraphs_generated >= max_paragraphs:
                        break
            elif word not in ['<EOS>', '<PAD>', '<UNK>']:
                current_paragraph.append(word)
            
            # Actualizar token actual
            current_token = torch.tensor([[next_token]]).to(DEVICE)
            
            # Terminar si EOS
            if word == '<EOS>' and len(story_parts) >= 2:
                break
    
    # Asegurar que tenemos al menos un p√°rrafo completo
    if current_paragraph:
        story_parts.append(' '.join(current_paragraph))
    
    # Formatear historia
    formatted_story = "\n\n".join(story_parts)
    
    # Limpiar y capitalizar
    formatted_story = re.sub(r'\s+([.,!?;:])', r'\1', formatted_story)
    formatted_story = re.sub(r'\s+', ' ', formatted_story)
    
    # Capitalizar oraciones
    def capitalize_sentences(text):
        sentences = re.split(r'([.!?] )', text)
        result = []
        for i in range(0, len(sentences)-1, 2):
            sentence = sentences[i].strip()
            punctuation = sentences[i+1] if i+1 < len(sentences) else ''
            if sentence:
                result.append(sentence[0].upper() + sentence[1:] + punctuation)
        return ' '.join(result)
    
    formatted_story = capitalize_sentences(formatted_story)
    
    return formatted_story

def generate_random_story_start(sentence_starts, word2idx):
    """Genera un inicio aleatorio basado en patrones aprendidos"""
    if not sentence_starts:
        # Inicio por defecto si no hay patrones
        return "The story begins"
    
    # Elegir un patr√≥n aleatorio
    start = random.choice(sentence_starts)
    
    # Asegurar que todas las palabras est√°n en el vocabulario
    words = start.split()
    valid_words = []
    
    for word in words:
        if word in word2idx or word.lower() in word2idx:
            valid_words.append(word)
        else:
            # Reemplazar con palabra similar o omitir
            for vocab_word in word2idx.keys():
                if len(vocab_word) > 3 and word[:3] == vocab_word[:3]:
                    valid_words.append(vocab_word)
                    break
    
    return ' '.join(valid_words[:5]) if valid_words else "Once upon a time"

# ===============================
# 7. ENTRENAMIENTO
# ===============================
def train_autonomous_model():
    """Entrena el modelo para generaci√≥n aut√≥noma"""
    print("\n=== ENTRENANDO MODELO AUT√ìNOMO ===")
    
    # Hiperpar√°metros
    SEQ_LENGTH = 60
    BATCH_SIZE = 128  # Mayor batch para m√°s estabilidad
    EMBED_DIM = 256
    HIDDEN_DIM = 512
    NUM_LAYERS = 2
    LEARNING_RATE = 0.002
    NUM_EPOCHS = 15
    
    # Dataset
    dataset = AutoStoryDataset(encoded_text, SEQ_LENGTH)
    dataloader = DataLoader(
        dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=0
    )
    
    print(f"Datos: {len(dataset)} secuencias de entrenamiento")
    print(f"Batch size: {BATCH_SIZE}, Sequence length: {SEQ_LENGTH}")
    
    # Modelo
    model = AutonomousStoryGenerator(
        vocab_size=vocab_size,
        embed_dim=EMBED_DIM,
        hidden_dim=HIDDEN_DIM,
        num_layers=NUM_LAYERS
    ).to(DEVICE)
    
    # Optimizador y scheduler
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.5)
    criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<PAD>'])
    
    print(f"\nModelo creado: {sum(p.numel() for p in model.parameters()):,} par√°metros")
    print("Comenzando entrenamiento...\n")
    
    # Entrenamiento
    best_loss = float('inf')
    
    for epoch in range(NUM_EPOCHS):
        model.train()
        total_loss = 0
        total_correct = 0
        total_tokens = 0
        
        for batch_idx, (x, y) in enumerate(dataloader):
            x, y = x.to(DEVICE), y.to(DEVICE)
            
            # Forward
            optimizer.zero_grad()
            output, _ = model(x)
            
            # Calcular p√©rdida
            loss = criterion(output, y)
            
            # Backward
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            # Estad√≠sticas
            total_loss += loss.item()
            
            # Calcular precisi√≥n
            _, predicted = torch.max(output, 1)
            total_correct += (predicted == y).sum().item()
            total_tokens += y.size(0)
            
            # Progreso
            if batch_idx % 200 == 0:
                accuracy = total_correct / total_tokens if total_tokens > 0 else 0
                print(f"Epoch {epoch+1:2d}/{NUM_EPOCHS} | "
                      f"Batch {batch_idx:4d} | "
                      f"Loss: {loss.item():.4f} | "
                      f"Acc: {accuracy:.2%}")
        
        # Estad√≠sticas de la √©poca
        avg_loss = total_loss / len(dataloader)
        accuracy = total_correct / total_tokens
        
        print(f"\n--- Epoch {epoch+1} completada ---")
        print(f"P√©rdida promedio: {avg_loss:.4f}")
        print(f"Precisi√≥n: {accuracy:.2%}")
        
        # Ajustar learning rate
        scheduler.step(avg_loss)
        
        # Generar muestra de historia (SIN prompt)
        print("\nüîÆ GENERANDO HISTORIA AUT√ìNOMA (sin prompt):")
        print("-" * 60)
        
        model.eval()
        autonomous_story = generate_autonomous_story(
            model, 
            word2idx, 
            idx2word, 
            length=400,
            temperature=0.7 + (epoch * 0.02),  # Aumentar temperatura gradualmente
            max_paragraphs=3
        )
        
        # Mostrar primeros 300 caracteres
        preview = autonomous_story[:300] + "..." if len(autonomous_story) > 300 else autonomous_story
        print(preview)
        print("-" * 60)
        
        # Guardar checkpoint si es el mejor
        if avg_loss < best_loss:
            best_loss = avg_loss
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_loss,
                'accuracy': accuracy,
                'vocab_size': vocab_size,
                'config': {
                    'embed_dim': EMBED_DIM,
                    'hidden_dim': HIDDEN_DIM,
                    'num_layers': NUM_LAYERS
                }
            }
            
            torch.save(checkpoint, os.path.join(CHECKPOINT_DIR, 'autonomous_story_model.pt'))
            print(f"‚úì Mejor modelo guardado (loss: {avg_loss:.4f})")
        
        print()
    
    return model

# ===============================
# 8. GENERACI√ìN DE M√öLTIPLES HISTORIAS
# ===============================
def generate_story_collection(model, num_stories=3):
    """Genera una colecci√≥n de historias completamente aut√≥nomas"""
    print("\n" + "="*60)
    print("GENERANDO COLECCI√ìN DE HISTORIAS")
    print("="*60)
    
    stories = []
    
    for i in range(num_stories):
        print(f"\nüìñ Generando historia {i+1}/{num_stories}...")
        
        # Configuraci√≥n variable para diversidad
        temperature = random.uniform(0.65, 0.85)
        length = random.randint(400, 700)
        paragraphs = random.randint(3, 5)
        
        # Generar historia SIN PROMPT
        story = generate_autonomous_story(
            model,
            word2idx,
            idx2word,
            length=length,
            temperature=temperature,
            max_paragraphs=paragraphs
        )
        
        # Guardar historia
        story_filename = f"autonomous_story_{i+1}_{random.randint(1000, 9999)}.txt"
        story_path = os.path.join(OUTPUT_DIR, story_filename)
        
        with open(story_path, 'w', encoding='utf-8') as f:
            f.write("=" * 60 + "\n")
            f.write(f"AUTONOMOUS GENERATED STORY #{i+1}\n")
            f.write(f"Temperature: {temperature:.2f}, Length: {length} tokens\n")
            f.write("=" * 60 + "\n\n")
            f.write(story)
            f.write("\n\n" + "=" * 60)
            f.write(f"\nGenerated by Autonomous LSTM+Attention Model\n")
            f.write(f"Trained on: {', '.join(used_files)}\n")
            f.write("=" * 60)
        
        stories.append({
            'number': i+1,
            'content': story,
            'filename': story_filename,
            'temperature': temperature,
            'length': length
        })
        
        # Mostrar preview
        print(f"Historia {i+1} generada ({len(story.split()):,} palabras)")
        print("-" * 50)
        
        # Mostrar primer p√°rrafo
        first_para = story.split('\n\n')[0] if '\n\n' in story else story[:200]
        print(first_para[:150] + "..." if len(first_para) > 150 else first_para)
        print()
    
    return stories

# ===============================
# 9. AN√ÅLISIS DE HISTORIAS GENERADAS
# ===============================
def analyze_generated_stories(stories):
    """Analiza las historias generadas"""
    print("\n" + "="*60)
    print("AN√ÅLISIS DE HISTORIAS GENERADAS")
    print("="*60)
    
    total_words = 0
    total_sentences = 0
    unique_words = set()
    
    for story_info in stories:
        story = story_info['content']
        words = story.split()
        sentences = re.split(r'[.!?]+', story)
        
        total_words += len(words)
        total_sentences += len([s for s in sentences if len(s.strip()) > 3])
        unique_words.update(words)
    
    print(f"\nEstad√≠sticas de {len(stories)} historias:")
    print(f"‚Ä¢ Total de palabras: {total_words:,}")
    print(f"‚Ä¢ Promedio por historia: {total_words/len(stories):.0f} palabras")
    print(f"‚Ä¢ Total de oraciones: {total_sentences:,}")
    print(f"‚Ä¢ Palabras √∫nicas usadas: {len(unique_words):,}")
    print(f"‚Ä¢ Riqueza l√©xica: {len(unique_words)/total_words*100:.1f}%")
    
    # Mostrar palabras m√°s usadas en las historias generadas
    all_words = ' '.join([s['content'] for s in stories]).split()
    word_counts = Counter(all_words)
    
    print(f"\n10 palabras m√°s comunes en historias generadas:")
    for word, count in word_counts.most_common(10):
        print(f"  {word}: {count}")
    
    print(f"\nLas historias se guardaron en: {OUTPUT_DIR}")

# ===============================
# 10. FUNCI√ìN PRINCIPAL
# ===============================
def main():
    """Flujo principal - completamente aut√≥nomo"""
    print("=" * 70)
    print("AUTONOMOUS STORY GENERATOR - NO PROMPTS NEEDED")
    print("LSTM + Attention Model - Generates from scratch")
    print("=" * 70)
    
    # Paso 1: Entrenar o cargar modelo
    print("\n1. PREPARANDO MODELO...")
    
    model_path = os.path.join(CHECKPOINT_DIR, 'autonomous_story_model.pt')
    
    if os.path.exists(model_path):
        choice = input("¬øEntrenar nuevo modelo (n) o usar existente (e)? [e]: ").lower()
        if choice == 'n':
            model = train_autonomous_model()
        else:
            # Cargar modelo existente
            print(f"Cargando modelo: {model_path}")
            checkpoint = torch.load(model_path, map_location=DEVICE)
            
            model = AutonomousStoryGenerator(
                vocab_size=vocab_size,
                embed_dim=checkpoint.get('config', {}).get('embed_dim', 256),
                hidden_dim=checkpoint.get('config', {}).get('hidden_dim', 512),
                num_layers=checkpoint.get('config', {}).get('num_layers', 2)
            ).to(DEVICE)
            
            model.load_state_dict(checkpoint['model_state_dict'])
            print(f"‚úì Modelo cargado (epoch {checkpoint.get('epoch', '?')}, "
                  f"loss: {checkpoint.get('loss', 0):.4f})")
    else:
        print("No hay modelo existente. Entrenando uno nuevo...")
        model = train_autonomous_model()
    
    # Paso 2: Generar historias
    print("\n2. GENERANDO HISTORIAS AUT√ìNOMAS...")
    
    num_stories = 3
    print(f"\nGenerando {num_stories} historias completamente aut√≥nomas...")
    print("(Sin prompts, sin entrada del usuario, puramente autom√°tico)")
    
    stories = generate_story_collection(model, num_stories)
    
    # Paso 3: An√°lisis
    analyze_generated_stories(stories)
    
    # Paso 4: Demostraci√≥n adicional
    print("\n3. DEMOSTRACI√ìN ADICIONAL...")
    print("Generando una historia m√°s larga como ejemplo final:")
    print("-" * 60)
    
    final_story = generate_autonomous_story(
        model,
        word2idx,
        idx2word,
        length=800,
        temperature=0.75,
        max_paragraphs=5
    )
    
    print(final_story[:500] + "..." if len(final_story) > 500 else final_story)
    print("-" * 60)
    
    # Guardar historia final
    final_path = os.path.join(OUTPUT_DIR, "final_demonstration_story.txt")
    with open(final_path, 'w', encoding='utf-8') as f:
        f.write(final_story)
    
    print(f"\n‚úì Historia de demostraci√≥n guardada en: {final_path}")
    
    # Resumen final
    print("\n" + "=" * 70)
    print("RESUMEN DEL SISTEMA AUT√ìNOMO")
    print("=" * 70)
    print("\n‚úÖ CARACTER√çSTICAS:")
    print("  ‚Ä¢ Genera historias COMPLETAMENTE AUT√ìNOMAS (sin prompts)")
    print("  ‚Ä¢ Modelo no-LLM: LSTM tradicional + Mecanismo de Atenci√≥n")
    print(f"  ‚Ä¢ Entrenado con: {len(encoded_text):,} tokens de texto")
    print(f"  ‚Ä¢ Vocabulario: {vocab_size} palabras")
    print(f"  ‚Ä¢ Arquitectura: {sum(p.numel() for p in model.parameters()):,} par√°metros")
    print(f"  ‚Ä¢ Historias generadas: {num_stories + 1} (m√°s demostraci√≥n)")
    print(f"  ‚Ä¢ Salida: {OUTPUT_DIR}/")
    
    print("\n‚úÖ VENTAJAS:")
    print("  ‚Ä¢ No requiere prompts ni entrada del usuario")
    print("  ‚Ä¢ Genera historias con estructura de p√°rrafos")
    print("  ‚Ä¢ Mantiene coherencia tem√°tica (aprendida de los datos)")
    print("  ‚Ä¢ Produce texto variado y creativo")
    print("  ‚Ä¢ Sistema completamente aut√≥nomo")
    
    print("\nüéØ CUMPLE REQUISITOS:")
    print("  ‚úì No es un LLM (modelo tradicional RNN)")
    print("  ‚úì Aprende de datos proporcionados")
    print("  ‚úì Genera historias cortas de varios p√°rrafos")
    print("  ‚úì Texto coherente (no aleatorio)")
    print("  ‚úì Completamente aut√≥nomo (sin prompts)")

# ===============================
# EJECUCI√ìN
# ===============================
if __name__ == "__main__":
    main()